{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New_MC VAE - VAE - Comparision.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdT9BTrmT3ro",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNUpL6YT32X",
        "colab_type": "code",
        "outputId": "d4f135c3-68d5-4444-d1ac-04af50f3266d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import set_random_seed\n",
        "\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.layers import Lambda, Input, Dense, Dropout\n",
        "\n",
        "GLOBAL_SEED = 1\n",
        "LOCAL_SEED = 42\n",
        "\n",
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFaXts2Ycg-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define PATH to file\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/breast-cancer-wisconsin/wdbc.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/balance-scale/balance-scale.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/pima-indians-diabetes/pima-indians-diabetes.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/tic-tac-toe/tic-tac-toe.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/annealing/anneal.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/breast-cancer/breast-cancer.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/cylinder-bands/bands.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/credit-screening/crx.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/statlog/australian/australian.dat'\n",
        "path = 'gdrive/My Drive/Generators/DataSets/Selected/statlog/german/german.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/statlog/german/german.data-numeric'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/spectrometer/lrs.data'\n",
        "# path = 'gdrive/My Drive/Generators/DataSets/Selected/soybean/soybean-large.data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8zWTy_ePGg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intermediate_dim = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VTGWwfObeoT",
        "colab_type": "text"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfcE15dwhyaL",
        "colab_type": "code",
        "outputId": "e63351a1-b48e-4b6a-80b2-9a61ef0c7675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "import pandas as pd\n",
        "na_values = {'?', ' '}\n",
        "df = pd.read_csv(path,\n",
        "                 sep=' ',\n",
        "                 header=None,\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "print('Origin dataset:')                 \n",
        "print(df.head())\n",
        "# Drop N/A \n",
        "df.dropna(axis=1, how='any', inplace=True)\n",
        "print(df.head())\n",
        "df.replace('U',np.NaN, inplace=True)\n",
        "# df.dropna(axis=0, how='any', inplace=True)\n",
        "# Drop ID column\n",
        "df.drop([0], axis=1, inplace=True)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "col_names = list(df)\n",
        "new_names = {}\n",
        "for i, name in enumerate(col_names):\n",
        "    new_names[name] = 'X' + str(i)\n",
        "df.rename(columns=new_names, inplace=True)\n",
        "# For soybean\n",
        "# colnums = len(df.columns)\n",
        "# for i in df.columns:\n",
        "#     df[i] = df[i].astype('category')\n",
        "\n",
        "# For Prima diabetes\n",
        "# df['X9'] = df['X9'].astype('category')\n",
        "# df['X8'] = df['X8'].astype('category')\n",
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Origin dataset:\n",
            "    0   1    2    3     4    5    6   ...    14 15    16  17    18    19 20\n",
            "0  A11   6  A34  A43  1169  A65  A75  ...  A152  2  A173   1  A192  A201  1\n",
            "1  A12  48  A32  A43  5951  A61  A73  ...  A152  1  A173   1  A191  A201  2\n",
            "2  A14  12  A34  A46  2096  A61  A74  ...  A152  1  A172   2  A191  A201  1\n",
            "3  A11  42  A32  A42  7882  A61  A74  ...  A153  1  A173   2  A191  A201  1\n",
            "4  A11  24  A33  A40  4870  A61  A73  ...  A153  2  A173   2  A191  A201  2\n",
            "\n",
            "[5 rows x 21 columns]\n",
            "    0   1    2    3     4    5    6   ...    14 15    16  17    18    19 20\n",
            "0  A11   6  A34  A43  1169  A65  A75  ...  A152  2  A173   1  A192  A201  1\n",
            "1  A12  48  A32  A43  5951  A61  A73  ...  A152  1  A173   1  A191  A201  2\n",
            "2  A14  12  A34  A46  2096  A61  A74  ...  A152  1  A172   2  A191  A201  1\n",
            "3  A11  42  A32  A42  7882  A61  A74  ...  A153  1  A173   2  A191  A201  1\n",
            "4  A11  24  A33  A40  4870  A61  A73  ...  A153  2  A173   2  A191  A201  2\n",
            "\n",
            "[5 rows x 21 columns]\n",
            "   X0   X1   X10  X11   X12   X13  X14  ...    X3   X4   X5 X6   X7    X8  X9\n",
            "0   6  A34  A121   67  A143  A152    2  ...  1169  A65  A75  4  A93  A101   4\n",
            "1  48  A32  A121   22  A143  A152    1  ...  5951  A61  A73  2  A92  A101   2\n",
            "2  12  A34  A121   49  A143  A152    1  ...  2096  A61  A74  2  A93  A101   3\n",
            "3  42  A32  A122   45  A143  A153    1  ...  7882  A61  A74  2  A93  A103   4\n",
            "4  24  A33  A124   53  A143  A153    2  ...  4870  A61  A73  3  A93  A101   4\n",
            "\n",
            "[5 rows x 20 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Yk-VnN1mlc",
        "colab_type": "text"
      },
      "source": [
        "# Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpGxC2v2CDi1",
        "colab_type": "code",
        "outputId": "ae36d14c-d154-40bc-d4f5-f50a38d30438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "vals = np.copy(df.values)\n",
        "total_nums = len(vals)\n",
        "\n",
        "df_train, df_validation = train_test_split(df, test_size=0.5, \n",
        "                                           random_state=LOCAL_SEED, \n",
        "                                           shuffle=True)\n",
        "# Write the test dataset\n",
        "df_validation = df_validation.reindex(sorted(df_validation.columns), axis=1)\n",
        "df_validation.to_csv(path + '_For_Test.csv', index=False)\n",
        "print(df_validation.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     X0   X1   X10  X11   X12   X13  X14  ...    X3   X4   X5 X6   X7    X8  X9\n",
            "521  18  A32  A121   24  A143  A152    1  ...  3190  A61  A73  2  A92  A101   2\n",
            "737  18  A32  A123   35  A143  A152    1  ...  4380  A62  A73  3  A93  A101   4\n",
            "740  24  A31  A123   32  A141  A152    1  ...  2325  A62  A74  2  A93  A101   3\n",
            "660  12  A32  A121   23  A143  A151    1  ...  1297  A61  A73  3  A94  A101   4\n",
            "411  33  A34  A123   35  A143  A152    2  ...  7253  A61  A74  3  A93  A101   2\n",
            "\n",
            "[5 rows x 20 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptmiXDEjbhmp",
        "colab_type": "text"
      },
      "source": [
        "# Recognize categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8c25594e-3d9f-4025-e8fc-7a5717041317",
        "id": "4nd-yR5cvyge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "df = df_train.copy(deep=True)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     X0   X1   X10  X11   X12   X13  X14  ...     X3   X4   X5 X6   X7    X8  X9\n",
            "680   6  A32  A124   56  A143  A152    1  ...   1538  A61  A72  1  A92  A101   2\n",
            "177   6  A34  A123   52  A143  A152    2  ...    338  A63  A75  4  A93  A101   4\n",
            "395  39  A33  A124   32  A143  A151    1  ...  11760  A62  A74  2  A93  A101   3\n",
            "911  24  A34  A123   25  A141  A152    1  ...   4736  A61  A72  2  A92  A101   4\n",
            "793  24  A32  A124   51  A143  A153    1  ...   2892  A61  A75  3  A91  A101   4\n",
            "\n",
            "[5 rows x 20 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfL6yO7HZR7E",
        "colab_type": "code",
        "outputId": "831e0386-ea97-47c4-92c5-6dd691c71f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# df['X9'] = df['X9'].astype('category')\n",
        "# df['X8'] = df['X8'].astype('category')\n",
        "colnums = len(df.columns)\n",
        "for i in df.columns:\n",
        "    try:\n",
        "        if df[i].dtype.name == 'object':\n",
        "            df[i] = df[i].astype('category')\n",
        "        else:\n",
        "            df[i].astype('float32')\n",
        "    except:\n",
        "        continue\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     X0   X1   X10  X11   X12   X13  X14  ...     X3   X4   X5 X6   X7    X8  X9\n",
            "680   6  A32  A124   56  A143  A152    1  ...   1538  A61  A72  1  A92  A101   2\n",
            "177   6  A34  A123   52  A143  A152    2  ...    338  A63  A75  4  A93  A101   4\n",
            "395  39  A33  A124   32  A143  A151    1  ...  11760  A62  A74  2  A93  A101   3\n",
            "911  24  A34  A123   25  A141  A152    1  ...   4736  A61  A72  2  A92  A101   4\n",
            "793  24  A32  A124   51  A143  A153    1  ...   2892  A61  A75  3  A91  A101   4\n",
            "\n",
            "[5 rows x 20 columns]\n",
            "               X0         X11         X14  ...            X3          X6          X9\n",
            "count  500.000000  500.000000  500.000000  ...    500.000000  500.000000  500.000000\n",
            "mean    21.452000   35.478000    1.416000  ...   3449.456000    3.002000    2.840000\n",
            "std     12.056634   11.299951    0.586201  ...   3073.550205    1.108356    1.084783\n",
            "min      4.000000   19.000000    1.000000  ...    250.000000    1.000000    1.000000\n",
            "25%     12.000000   27.000000    1.000000  ...   1364.000000    2.000000    2.000000\n",
            "50%     18.000000   33.000000    1.000000  ...   2335.000000    3.000000    3.000000\n",
            "75%     24.000000   42.000000    2.000000  ...   4274.000000    4.000000    4.000000\n",
            "max     60.000000   75.000000    4.000000  ...  18424.000000    4.000000    4.000000\n",
            "\n",
            "[8 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-52cnSrZHGm",
        "colab_type": "code",
        "outputId": "9a140220-4f18-4d08-bae5-6e31f80a5a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# df['X9'] = df['X9'].astype('category')\n",
        "# df['X8'] = df['X8'].astype('category')\n",
        "categorical = df.select_dtypes(['category']).columns\n",
        "print(categorical)\n",
        "for f in categorical:\n",
        "    dummies = pd.get_dummies(df[f], prefix = f, prefix_sep = '_')\n",
        "    df = pd.concat([df, dummies], axis = 1)\n",
        "    \n",
        "# drop original categorical features\n",
        "df.drop(categorical, axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['X1', 'X10', 'X12', 'X13', 'X15', 'X17', 'X18', 'X2', 'X4', 'X5', 'X7',\n",
            "       'X8'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9M4zr70tl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(path + 'For_training.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnJXo9LfUGaJ",
        "colab_type": "text"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYeYLHK2fpXX",
        "colab_type": "text"
      },
      "source": [
        "## Split train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmIPmd5wjAM7",
        "colab_type": "code",
        "outputId": "41a3dc6f-a5dd-4b00-9de8-e15e6039cf84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "df = pd.read_csv(path + 'For_training.csv')\n",
        "vae_train = np.copy(df.values)\n",
        "vae_train.astype('float32')\n",
        "scaler = MinMaxScaler()\n",
        "print(np.amax(vae_train[:, 2]))\n",
        "\n",
        "vae_train = scaler.fit_transform(vae_train)\n",
        "x_train, x_test = train_test_split(vae_train, test_size=0.5,\n",
        "                                   random_state=LOCAL_SEED,\n",
        "                                   shuffle=True)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(np.amax(x_train))\n",
        "print(np.amax(x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "(250, 58)\n",
            "(250, 58)\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNf-3EMl2usm",
        "colab_type": "code",
        "outputId": "660f949d-0cef-49bc-c93d-f107d1eb7205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "original_dim = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 58)\n",
            "(250, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2vq5zxdfvq7",
        "colab_type": "text"
      },
      "source": [
        "## Define VAE class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy-JqW7Jpuyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "class VAE:\n",
        "    def __init__(self, input_shape=(original_dim,), \n",
        "                 intermediate_dim=128, latent_dim=2, summary=False):\n",
        "        \n",
        "        self._build_model(input_shape,\n",
        "                         intermediate_dim, \n",
        "                          latent_dim, summary)\n",
        "    \n",
        "    def _build_model(self, input_shape, intermediate_dim, latent_dim,\n",
        "                    summary=False):\n",
        "        inputs = Input(shape=input_shape, name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(intermediate_dim, activation='relu')(x)\n",
        "        x = Dense(intermediate_dim//2, activation='relu')(x)\n",
        "        \n",
        "        z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "        z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "        z = Lambda(self.sampling, output_shape=(latent_dim,), \n",
        "                   name='z')([z_mean, z_log_var])\n",
        "\n",
        "        self.encoder = Model(inputs, [z_mean, z_log_var, z], \n",
        "                        name='encoder')\n",
        "        \n",
        "        latent_inputs = Input(shape=(latent_dim,), \n",
        "                              name='z_sampling')\n",
        "        x = latent_inputs\n",
        "        x = Dense(intermediate_dim//2, activation='relu')(x)\n",
        "        x = Dense(intermediate_dim, activation='relu')(x)\n",
        "        outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "        outputs = self.decoder(self.encoder(inputs)[2])\n",
        "        self.vae = Model(inputs, outputs, name='vae_mlp')\n",
        "        \n",
        "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
        "        reconstruction_loss *= original_dim\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        \n",
        "        vae_loss = K.mean(reconstruction_loss + kl_loss)\t\n",
        "        \n",
        "        self.vae.add_loss(vae_loss)\n",
        "        self.vae.compile(optimizer='adam')\n",
        "        if summary: \n",
        "            print(self.vae.summary())\n",
        "        \n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        dim = K.int_shape(z_mean)[1]\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "        \n",
        "    def fit(self, x_train, x_test, epochs=100, batch_size=100,\n",
        "           verbose=1):\n",
        "        self.vae.fit(x_train, \n",
        "            shuffle=True,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=verbose,\n",
        "            validation_data=(x_test, None))\n",
        "    \n",
        "    def encoder_predict(self, x_test, batch_size=100):\n",
        "        return self.encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    \n",
        "    def generate(self, latent_val, batch_size=100):\n",
        "        return self.decoder.predict(latent_val)\n",
        "    \n",
        "    def predict(self, x_test, batch_size=1):\n",
        "        prediction = self.vae.predict(x_test)\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXhRjx02UYZN",
        "colab_type": "text"
      },
      "source": [
        "## Training VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd02caRJWGG8",
        "colab_type": "text"
      },
      "source": [
        "Just let the last value to test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIQUNfHLkey_",
        "colab_type": "code",
        "outputId": "ac665a43-b222-49ff-d71c-1c04c420a401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(np.amax(x_train))\n",
        "print(np.amin(x_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 58)\n",
            "1.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBbIF17p8iY2",
        "colab_type": "code",
        "outputId": "376a3e1d-871c-4ddd-dd62-b334d48d75d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5256
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "latent_dim = original_dim//2\n",
        "if latent_dim < 2:\n",
        "    latent_dim = 2\n",
        "vae = VAE(intermediate_dim=intermediate_dim, latent_dim=latent_dim)\n",
        "vae.fit(x_train, x_test, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 250 samples, validate on 250 samples\n",
            "Epoch 1/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 39.0254 - val_loss: 33.9612\n",
            "Epoch 2/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 32.3522 - val_loss: 28.3470\n",
            "Epoch 3/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 27.5920 - val_loss: 25.8005\n",
            "Epoch 4/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 25.8128 - val_loss: 25.0729\n",
            "Epoch 5/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 25.2504 - val_loss: 24.8825\n",
            "Epoch 6/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 24.9383 - val_loss: 24.5453\n",
            "Epoch 7/150\n",
            "250/250 [==============================] - 0s 75us/step - loss: 24.8867 - val_loss: 24.4352\n",
            "Epoch 8/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 24.8148 - val_loss: 24.2494\n",
            "Epoch 9/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 24.7382 - val_loss: 24.2377\n",
            "Epoch 10/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 24.6313 - val_loss: 24.1545\n",
            "Epoch 11/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 24.5066 - val_loss: 24.1242\n",
            "Epoch 12/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 24.4624 - val_loss: 24.0557\n",
            "Epoch 13/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 24.3133 - val_loss: 24.0472\n",
            "Epoch 14/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 24.4100 - val_loss: 24.0485\n",
            "Epoch 15/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 24.3623 - val_loss: 24.1004\n",
            "Epoch 16/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 24.4685 - val_loss: 24.0593\n",
            "Epoch 17/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 24.2885 - val_loss: 24.0787\n",
            "Epoch 18/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 24.3274 - val_loss: 23.9233\n",
            "Epoch 19/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 24.3174 - val_loss: 24.0717\n",
            "Epoch 20/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 24.3218 - val_loss: 24.0348\n",
            "Epoch 21/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 24.2979 - val_loss: 23.9976\n",
            "Epoch 22/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 24.2103 - val_loss: 24.0120\n",
            "Epoch 23/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 24.3086 - val_loss: 23.9738\n",
            "Epoch 24/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 24.1769 - val_loss: 23.8973\n",
            "Epoch 25/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 24.1301 - val_loss: 23.9810\n",
            "Epoch 26/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 24.3534 - val_loss: 23.8757\n",
            "Epoch 27/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 24.2237 - val_loss: 23.8957\n",
            "Epoch 28/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 24.2787 - val_loss: 23.9771\n",
            "Epoch 29/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 24.1589 - val_loss: 23.9207\n",
            "Epoch 30/150\n",
            "250/250 [==============================] - 0s 109us/step - loss: 24.2001 - val_loss: 23.9649\n",
            "Epoch 31/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 24.2063 - val_loss: 23.8089\n",
            "Epoch 32/150\n",
            "250/250 [==============================] - 0s 74us/step - loss: 24.3147 - val_loss: 23.9193\n",
            "Epoch 33/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 24.0937 - val_loss: 23.8465\n",
            "Epoch 34/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 24.0845 - val_loss: 23.8848\n",
            "Epoch 35/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 24.1687 - val_loss: 23.8858\n",
            "Epoch 36/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 24.1759 - val_loss: 23.7877\n",
            "Epoch 37/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 24.1545 - val_loss: 23.9023\n",
            "Epoch 38/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 24.1443 - val_loss: 23.8189\n",
            "Epoch 39/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 24.0691 - val_loss: 23.9740\n",
            "Epoch 40/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 24.0772 - val_loss: 23.8763\n",
            "Epoch 41/150\n",
            "250/250 [==============================] - 0s 79us/step - loss: 24.0183 - val_loss: 23.9474\n",
            "Epoch 42/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 24.1475 - val_loss: 23.8874\n",
            "Epoch 43/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 24.1879 - val_loss: 23.8501\n",
            "Epoch 44/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 24.1020 - val_loss: 23.8267\n",
            "Epoch 45/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 23.9779 - val_loss: 23.6002\n",
            "Epoch 46/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 23.7934 - val_loss: 23.6125\n",
            "Epoch 47/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 23.8655 - val_loss: 23.6138\n",
            "Epoch 48/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 23.8085 - val_loss: 23.7282\n",
            "Epoch 49/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 23.8199 - val_loss: 23.5980\n",
            "Epoch 50/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 23.9126 - val_loss: 23.5000\n",
            "Epoch 51/150\n",
            "250/250 [==============================] - 0s 73us/step - loss: 23.7740 - val_loss: 23.4756\n",
            "Epoch 52/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 23.6623 - val_loss: 23.3553\n",
            "Epoch 53/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 23.6560 - val_loss: 23.3741\n",
            "Epoch 54/150\n",
            "250/250 [==============================] - 0s 75us/step - loss: 23.6930 - val_loss: 23.4583\n",
            "Epoch 55/150\n",
            "250/250 [==============================] - 0s 79us/step - loss: 23.5530 - val_loss: 23.2923\n",
            "Epoch 56/150\n",
            "250/250 [==============================] - 0s 97us/step - loss: 23.5392 - val_loss: 23.3555\n",
            "Epoch 57/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 23.6043 - val_loss: 23.4732\n",
            "Epoch 58/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 23.4633 - val_loss: 23.4100\n",
            "Epoch 59/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 23.3533 - val_loss: 23.3166\n",
            "Epoch 60/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 23.3619 - val_loss: 23.3193\n",
            "Epoch 61/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.4494 - val_loss: 23.0492\n",
            "Epoch 62/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 23.2655 - val_loss: 23.1059\n",
            "Epoch 63/150\n",
            "250/250 [==============================] - 0s 103us/step - loss: 23.2905 - val_loss: 23.2651\n",
            "Epoch 64/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 23.1822 - val_loss: 23.1924\n",
            "Epoch 65/150\n",
            "250/250 [==============================] - 0s 79us/step - loss: 23.3428 - val_loss: 23.3887\n",
            "Epoch 66/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 23.3070 - val_loss: 23.4694\n",
            "Epoch 67/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 23.0771 - val_loss: 23.5463\n",
            "Epoch 68/150\n",
            "250/250 [==============================] - 0s 75us/step - loss: 23.2231 - val_loss: 23.1008\n",
            "Epoch 69/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 23.0356 - val_loss: 23.1961\n",
            "Epoch 70/150\n",
            "250/250 [==============================] - 0s 70us/step - loss: 23.1150 - val_loss: 23.0661\n",
            "Epoch 71/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 23.0439 - val_loss: 23.2127\n",
            "Epoch 72/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 22.8225 - val_loss: 23.1628\n",
            "Epoch 73/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 22.9606 - val_loss: 23.2330\n",
            "Epoch 74/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 23.0672 - val_loss: 22.9772\n",
            "Epoch 75/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 23.2135 - val_loss: 23.1179\n",
            "Epoch 76/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 22.9249 - val_loss: 23.3737\n",
            "Epoch 77/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 22.9215 - val_loss: 23.3662\n",
            "Epoch 78/150\n",
            "250/250 [==============================] - 0s 126us/step - loss: 22.9344 - val_loss: 23.0569\n",
            "Epoch 79/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 22.9934 - val_loss: 22.8429\n",
            "Epoch 80/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 22.7943 - val_loss: 23.0785\n",
            "Epoch 81/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 22.7681 - val_loss: 23.0486\n",
            "Epoch 82/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 22.8216 - val_loss: 23.1078\n",
            "Epoch 83/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 22.9521 - val_loss: 23.4285\n",
            "Epoch 84/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 22.7281 - val_loss: 23.1060\n",
            "Epoch 85/150\n",
            "250/250 [==============================] - 0s 74us/step - loss: 22.8126 - val_loss: 23.0292\n",
            "Epoch 86/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.0074 - val_loss: 23.1874\n",
            "Epoch 87/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 22.7399 - val_loss: 23.3110\n",
            "Epoch 88/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 22.8474 - val_loss: 23.2193\n",
            "Epoch 89/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 22.6995 - val_loss: 23.1805\n",
            "Epoch 90/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 22.5999 - val_loss: 22.9526\n",
            "Epoch 91/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 22.5782 - val_loss: 22.8411\n",
            "Epoch 92/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 22.7963 - val_loss: 23.2146\n",
            "Epoch 93/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 22.4737 - val_loss: 22.8899\n",
            "Epoch 94/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 22.5574 - val_loss: 22.8230\n",
            "Epoch 95/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 22.5377 - val_loss: 22.8082\n",
            "Epoch 96/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 22.3997 - val_loss: 22.9575\n",
            "Epoch 97/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 22.1838 - val_loss: 23.0883\n",
            "Epoch 98/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 22.2341 - val_loss: 22.8877\n",
            "Epoch 99/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 22.4921 - val_loss: 23.0536\n",
            "Epoch 100/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 22.7048 - val_loss: 23.0175\n",
            "Epoch 101/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 22.4884 - val_loss: 22.9303\n",
            "Epoch 102/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 22.6569 - val_loss: 22.8918\n",
            "Epoch 103/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 22.4483 - val_loss: 22.8944\n",
            "Epoch 104/150\n",
            "250/250 [==============================] - 0s 79us/step - loss: 22.1721 - val_loss: 23.2904\n",
            "Epoch 105/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 22.4644 - val_loss: 22.9958\n",
            "Epoch 106/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 22.2912 - val_loss: 23.1820\n",
            "Epoch 107/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 22.0190 - val_loss: 22.8442\n",
            "Epoch 108/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 22.2565 - val_loss: 22.8040\n",
            "Epoch 109/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 22.0563 - val_loss: 22.8192\n",
            "Epoch 110/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 22.3298 - val_loss: 22.8763\n",
            "Epoch 111/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 22.0743 - val_loss: 22.9683\n",
            "Epoch 112/150\n",
            "250/250 [==============================] - 0s 72us/step - loss: 22.2701 - val_loss: 22.9137\n",
            "Epoch 113/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 22.3918 - val_loss: 22.9584\n",
            "Epoch 114/150\n",
            "250/250 [==============================] - 0s 73us/step - loss: 22.0952 - val_loss: 22.9761\n",
            "Epoch 115/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 22.0474 - val_loss: 22.3604\n",
            "Epoch 116/150\n",
            "250/250 [==============================] - 0s 79us/step - loss: 22.0518 - val_loss: 22.7323\n",
            "Epoch 117/150\n",
            "250/250 [==============================] - 0s 73us/step - loss: 21.9838 - val_loss: 22.7964\n",
            "Epoch 118/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 22.0547 - val_loss: 22.5168\n",
            "Epoch 119/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 21.9580 - val_loss: 22.7801\n",
            "Epoch 120/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 21.8624 - val_loss: 22.8607\n",
            "Epoch 121/150\n",
            "250/250 [==============================] - 0s 79us/step - loss: 21.6592 - val_loss: 22.2895\n",
            "Epoch 122/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 21.8038 - val_loss: 22.8112\n",
            "Epoch 123/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 21.7530 - val_loss: 22.6363\n",
            "Epoch 124/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 21.7129 - val_loss: 22.7457\n",
            "Epoch 125/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 21.7539 - val_loss: 22.7549\n",
            "Epoch 126/150\n",
            "250/250 [==============================] - 0s 112us/step - loss: 21.7091 - val_loss: 22.9286\n",
            "Epoch 127/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 21.7489 - val_loss: 22.5698\n",
            "Epoch 128/150\n",
            "250/250 [==============================] - 0s 80us/step - loss: 21.8932 - val_loss: 22.8134\n",
            "Epoch 129/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 21.6000 - val_loss: 22.6289\n",
            "Epoch 130/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 21.6435 - val_loss: 22.7420\n",
            "Epoch 131/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 21.6068 - val_loss: 22.6963\n",
            "Epoch 132/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 21.8820 - val_loss: 22.5729\n",
            "Epoch 133/150\n",
            "250/250 [==============================] - 0s 73us/step - loss: 21.6654 - val_loss: 22.8520\n",
            "Epoch 134/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 21.3007 - val_loss: 22.5104\n",
            "Epoch 135/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 21.4502 - val_loss: 22.7990\n",
            "Epoch 136/150\n",
            "250/250 [==============================] - 0s 73us/step - loss: 21.5749 - val_loss: 23.0955\n",
            "Epoch 137/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 21.6376 - val_loss: 22.6719\n",
            "Epoch 138/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 21.2492 - val_loss: 22.5968\n",
            "Epoch 139/150\n",
            "250/250 [==============================] - 0s 72us/step - loss: 21.2430 - val_loss: 22.5583\n",
            "Epoch 140/150\n",
            "250/250 [==============================] - 0s 79us/step - loss: 21.3045 - val_loss: 23.0594\n",
            "Epoch 141/150\n",
            "250/250 [==============================] - 0s 75us/step - loss: 21.3257 - val_loss: 22.7530\n",
            "Epoch 142/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 21.0428 - val_loss: 22.5360\n",
            "Epoch 143/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 21.3383 - val_loss: 22.6781\n",
            "Epoch 144/150\n",
            "250/250 [==============================] - 0s 77us/step - loss: 21.2797 - val_loss: 22.4158\n",
            "Epoch 145/150\n",
            "250/250 [==============================] - 0s 78us/step - loss: 21.2431 - val_loss: 22.8226\n",
            "Epoch 146/150\n",
            "250/250 [==============================] - 0s 74us/step - loss: 21.4550 - val_loss: 22.6363\n",
            "Epoch 147/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 21.2748 - val_loss: 22.6345\n",
            "Epoch 148/150\n",
            "250/250 [==============================] - 0s 76us/step - loss: 21.3000 - val_loss: 22.6463\n",
            "Epoch 149/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 21.1036 - val_loss: 22.7424\n",
            "Epoch 150/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 21.0710 - val_loss: 22.7480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVjp817qf1VL",
        "colab_type": "text"
      },
      "source": [
        "## Generate data with VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiV2iQyVwSdN",
        "colab_type": "code",
        "outputId": "de4c7c1a-df64-4955-8e31-568938f21a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, original_dim))\n",
        "x_test_encoded = vae.encoder.predict(x_test)\n",
        "x_test_encoded = np.asarray(x_test_encoded)\n",
        "\n",
        "print(x_test_encoded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 250, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saTZnq9pCZpd",
        "colab_type": "text"
      },
      "source": [
        "## Computing time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "304e52ad-2aea-4b8e-86e6-c458cabd4b15",
        "id": "Jx4Zbw_pCADa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "computation = []\n",
        "for _ in range(10):\n",
        "    start = time.time()\n",
        "    total_nums = 2\n",
        "    results = []\n",
        "    for i in range(x_test_encoded.shape[1]):\n",
        "        latent_gen = []\n",
        "        for _ in range(total_nums):\n",
        "            epsilon = np.random.normal(0., 1., x_test_encoded.shape[2])\n",
        "            latent_gen.extend([x_test_encoded[0, i, :] + np.exp(x_test_encoded[1, i, :]*0.5)*epsilon])\n",
        "        latent_gen = np.asarray(latent_gen)\n",
        "        results.append(vae.generate(latent_gen))\n",
        "\n",
        "    results = np.asarray(results)\n",
        "    results = np.reshape(results, (-1, original_dim))\n",
        "    results = scaler.inverse_transform(results)\n",
        "\n",
        "    end = time.time()\n",
        "    computation.append(end-start)\n",
        "print(np.mean(computation), np.std(computation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.17435188293457032 0.00878902653198987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTfvSN_R2c76",
        "colab_type": "code",
        "outputId": "a4c39df7-5b79-4e2d-9176-a52e8c60fc56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "total_nums = 2\n",
        "results = []\n",
        "for i in range(x_test_encoded.shape[1]):\n",
        "    latent_gen = []\n",
        "    for _ in range(total_nums):\n",
        "        epsilon = np.random.normal(0., 1., x_test_encoded.shape[2])\n",
        "        latent_gen.extend([x_test_encoded[0, i, :] + np.exp(x_test_encoded[1, i, :]*0.5)*epsilon])\n",
        "    latent_gen = np.asarray(latent_gen)\n",
        "    results.append(vae.generate(latent_gen))\n",
        "    \n",
        "results = np.asarray(results)\n",
        "results = np.reshape(results, (-1, original_dim))\n",
        "print(results.shape)\n",
        "results = scaler.inverse_transform(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo_Sfu9rgh8X",
        "colab_type": "text"
      },
      "source": [
        "## Handling generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdg-w-3eMquu",
        "colab_type": "code",
        "outputId": "31d541c0-630d-488a-fe1e-5384523443ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(results[:, 1]))\n",
        "print(results[0, 0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "19.706902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC6DiGV8MV9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {}\n",
        "names = list(df)\n",
        "for i, name in enumerate(names):\n",
        "    d[name] = results[:, i]\n",
        "df = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvdePB8gp1k",
        "colab_type": "text"
      },
      "source": [
        "## Re-categorical columns from generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49_bbWWCsrHG",
        "colab_type": "code",
        "outputId": "31655091-1ca0-4b96-cbb1-e01d063b21c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "names = list(df)\n",
        "c_dict = {}\n",
        "for n in names:\n",
        "    if '_' in n:\n",
        "        index = n.index('_')\n",
        "        c_dict[n[:index]] = [c for c in names if n[:index+1] in c]\n",
        "values = []\n",
        "for key, items in c_dict.items():\n",
        "    dummies = df[items]\n",
        "    d_names = list(dummies)\n",
        "    c_dict = {}\n",
        "    for n in d_names:\n",
        "        c_dict[n] = n[n.index('_')+1:]\n",
        "    dummies.rename(columns=c_dict, \n",
        "                   inplace=True)\n",
        "    df[key] = dummies.idxmax(axis=1)\n",
        "    df.drop(items, axis=1, inplace=True)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  return super(DataFrame, self).rename(**kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "          X0        X11       X14       X16       X19  ...   X2   X4   X5   X7    X8\n",
            "0  19.706902  38.357006  1.294747  1.025787  1.067945  ...  A43  A65  A72  A92  A101\n",
            "1  18.419031  34.398422  1.324374  1.005644  1.103526  ...  A42  A61  A72  A92  A101\n",
            "2  23.244650  37.615776  1.678816  1.129324  1.262234  ...  A43  A65  A75  A93  A101\n",
            "3  20.215624  37.397747  1.623275  1.133832  1.373835  ...  A42  A61  A72  A93  A101\n",
            "4  21.632650  49.568626  2.123373  1.500253  1.052936  ...  A40  A61  A75  A93  A101\n",
            "\n",
            "[5 rows x 20 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ljK-ceANBih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "df.to_csv(path + '_vae.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SZr05ADu6LQP"
      },
      "source": [
        "# Dropout VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VSYitqlg1zw",
        "colab_type": "text"
      },
      "source": [
        "## Split train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b68406f1-66a6-4274-d102-46a53670aab2",
        "id": "oNqkHN9sxOm_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df = pd.read_csv(path + 'For_training.csv')\n",
        "train = np.copy(df.values)\n",
        "train.astype('float32')\n",
        "scaler = MinMaxScaler()\n",
        "print(np.amax(train[:, 2]))\n",
        "\n",
        "train = scaler.fit_transform(train)\n",
        "x_train, x_test = train_test_split(train, test_size=0.5,\n",
        "                                  random_state=LOCAL_SEED,\n",
        "                                  shuffle=True)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(np.amax(x_train))\n",
        "print(np.amax(x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "(250, 58)\n",
            "(250, 58)\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e899807c-d364-4ad0-eb6b-9b91a68a8a53",
        "id": "Kpetr9JKxOnK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "original_dim = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 58)\n",
            "(250, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX39keJ8g6sz",
        "colab_type": "text"
      },
      "source": [
        "## Define Dropout VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MnKFnBX-6LQR",
        "colab": {}
      },
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.losses import categorical_crossentropy\n",
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "class DropoutVAE:\n",
        "    def __init__(self, input_shape=(original_dim,), \n",
        "                 intermediate_dim=32, latent_dim=3, dropout=0.05, \n",
        "                 summary=False):\n",
        "        \n",
        "        self._build_model(input_shape,\n",
        "                         intermediate_dim, \n",
        "                          latent_dim, summary,\n",
        "                          dropout)\n",
        "    \n",
        "    def _build_model(self, input_shape, intermediate_dim, latent_dim,\n",
        "                    summary=False, dropout=0.05):\n",
        "        inputs = Input(shape=input_shape, name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(intermediate_dim, activation='relu')(x)\n",
        "        x = Dense(intermediate_dim//2, activation='relu')(x)\n",
        "        \n",
        "        z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "        z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "        z = Lambda(self.sampling, output_shape=(latent_dim,), \n",
        "                   name='z')([z_mean, z_log_var])\n",
        "\n",
        "        self.encoder = Model(inputs, [z_mean, z_log_var, z], \n",
        "                        name='encoder')\n",
        "        \n",
        "        latent_inputs = Input(shape=(latent_dim,), \n",
        "                              name='z_sampling')\n",
        "        x = latent_inputs\n",
        "        x = Dense(intermediate_dim//2, activation='relu',\n",
        "                 kernel_regularizer=l2(1e-4),\n",
        "                 bias_regularizer=l2(1e-4))(x)\n",
        "        x = Dropout(dropout)(x)\n",
        "        x = Dense(intermediate_dim, activation='relu',\n",
        "                 kernel_regularizer=l2(1e-4),\n",
        "                 bias_regularizer=l2(1e-4))(x)\n",
        "        x = Dropout(dropout)(x)\n",
        "        outputs = Dense(original_dim, activation='sigmoid',\n",
        "                       kernel_regularizer=l2(1e-4),\n",
        "                       bias_regularizer=l2(1e-4))(x)\n",
        "\n",
        "        self.decoder = Model(latent_inputs, \n",
        "                             outputs, \n",
        "                             name='decoder')\n",
        "        outputs = self.decoder(self.encoder(inputs)[2])\n",
        "        self.vae = Model(inputs, outputs, \n",
        "                         name='vae_mlp')\n",
        "        \n",
        "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
        "        reconstruction_loss *= original_dim\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        \n",
        "        vae_loss = K.mean(reconstruction_loss + kl_loss)\t\n",
        "        \n",
        "        self.vae.add_loss(vae_loss)\n",
        "        self.vae.compile(optimizer='adam')\n",
        "        if summary: \n",
        "            print(self.vae.summary())\n",
        "        \n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        dim = K.int_shape(z_mean)[1]\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "        \n",
        "    def fit(self, x_train, x_test, epochs=100, batch_size=100,\n",
        "           verbose=1):\n",
        "        self.vae.fit(x_train, \n",
        "            shuffle=True,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=verbose,\n",
        "            validation_data=(x_test, None))\n",
        "    \n",
        "    def encoder_predict(self, x_test, batch_size=100):\n",
        "        return self.encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    \n",
        "    def generate(self, latent_val, batch_size=100):\n",
        "        return self.decoder.predict(latent_val)\n",
        "    \n",
        "    def predict(self, x_test, batch_size=1, nums=1000):\n",
        "        Yt_hat = []\n",
        "        for _ in range(nums):\n",
        "            Yt_hat.extend(self.vae.predict(x_test))\n",
        "                          \n",
        "        return np.asarray(Yt_hat)\n",
        "                          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TsARTKz36LQZ"
      },
      "source": [
        "## Train and evaluate Dropout VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkdmVggK6LQp",
        "outputId": "608c063e-87ab-42e4-ae2c-4ba96b02a899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5443
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "latent_dim = original_dim//2\n",
        "if latent_dim < 2:\n",
        "    latent_dim = 2\n",
        "vae = DropoutVAE(intermediate_dim=intermediate_dim,\n",
        "                 dropout=0.2, latent_dim=latent_dim,\n",
        "                 summary=True)\n",
        "vae.fit(x_train, x_test, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 58)                0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 29), (None, 29),  176442    \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 58)                169018    \n",
            "=================================================================\n",
            "Total params: 345,460\n",
            "Trainable params: 345,460\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 250 samples, validate on 250 samples\n",
            "Epoch 1/150\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 39.8268 - val_loss: 35.2467\n",
            "Epoch 2/150\n",
            "250/250 [==============================] - 0s 97us/step - loss: 33.6920 - val_loss: 29.7580\n",
            "Epoch 3/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 28.8787 - val_loss: 26.1593\n",
            "Epoch 4/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 26.2561 - val_loss: 25.1516\n",
            "Epoch 5/150\n",
            "250/250 [==============================] - 0s 99us/step - loss: 25.6071 - val_loss: 24.7091\n",
            "Epoch 6/150\n",
            "250/250 [==============================] - 0s 112us/step - loss: 25.3077 - val_loss: 24.5272\n",
            "Epoch 7/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 25.1971 - val_loss: 24.4637\n",
            "Epoch 8/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 25.1959 - val_loss: 24.3591\n",
            "Epoch 9/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 25.0683 - val_loss: 24.3110\n",
            "Epoch 10/150\n",
            "250/250 [==============================] - 0s 99us/step - loss: 25.0108 - val_loss: 24.2318\n",
            "Epoch 11/150\n",
            "250/250 [==============================] - 0s 100us/step - loss: 24.7098 - val_loss: 24.1680\n",
            "Epoch 12/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 24.6567 - val_loss: 24.0924\n",
            "Epoch 13/150\n",
            "250/250 [==============================] - 0s 97us/step - loss: 24.8386 - val_loss: 24.0646\n",
            "Epoch 14/150\n",
            "250/250 [==============================] - 0s 100us/step - loss: 24.7756 - val_loss: 24.1354\n",
            "Epoch 15/150\n",
            "250/250 [==============================] - 0s 101us/step - loss: 24.7529 - val_loss: 24.0733\n",
            "Epoch 16/150\n",
            "250/250 [==============================] - 0s 102us/step - loss: 24.6324 - val_loss: 24.1862\n",
            "Epoch 17/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 24.5899 - val_loss: 24.0863\n",
            "Epoch 18/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 24.5796 - val_loss: 24.1142\n",
            "Epoch 19/150\n",
            "250/250 [==============================] - 0s 99us/step - loss: 24.5531 - val_loss: 24.1299\n",
            "Epoch 20/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 24.6652 - val_loss: 24.0799\n",
            "Epoch 21/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 24.6687 - val_loss: 24.1259\n",
            "Epoch 22/150\n",
            "250/250 [==============================] - 0s 97us/step - loss: 24.5860 - val_loss: 24.0225\n",
            "Epoch 23/150\n",
            "250/250 [==============================] - 0s 123us/step - loss: 24.5964 - val_loss: 23.9238\n",
            "Epoch 24/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 24.5394 - val_loss: 24.0286\n",
            "Epoch 25/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 24.5292 - val_loss: 24.0655\n",
            "Epoch 26/150\n",
            "250/250 [==============================] - 0s 98us/step - loss: 24.5869 - val_loss: 23.9961\n",
            "Epoch 27/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 24.4445 - val_loss: 24.0071\n",
            "Epoch 28/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 24.4513 - val_loss: 24.0093\n",
            "Epoch 29/150\n",
            "250/250 [==============================] - 0s 100us/step - loss: 24.5479 - val_loss: 23.9485\n",
            "Epoch 30/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 24.4948 - val_loss: 24.0029\n",
            "Epoch 31/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 24.3926 - val_loss: 23.8987\n",
            "Epoch 32/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 24.5625 - val_loss: 23.8426\n",
            "Epoch 33/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 24.3972 - val_loss: 23.9441\n",
            "Epoch 34/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 24.4107 - val_loss: 23.9374\n",
            "Epoch 35/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 24.2969 - val_loss: 23.8966\n",
            "Epoch 36/150\n",
            "250/250 [==============================] - 0s 116us/step - loss: 24.2740 - val_loss: 23.9033\n",
            "Epoch 37/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 24.4310 - val_loss: 23.8160\n",
            "Epoch 38/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 24.4871 - val_loss: 23.9179\n",
            "Epoch 39/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 24.3903 - val_loss: 23.8864\n",
            "Epoch 40/150\n",
            "250/250 [==============================] - 0s 99us/step - loss: 24.4192 - val_loss: 23.8666\n",
            "Epoch 41/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 24.2927 - val_loss: 23.7927\n",
            "Epoch 42/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 24.2467 - val_loss: 23.8424\n",
            "Epoch 43/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 24.3405 - val_loss: 23.7556\n",
            "Epoch 44/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 24.3124 - val_loss: 23.8072\n",
            "Epoch 45/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 24.2845 - val_loss: 23.7564\n",
            "Epoch 46/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 24.2845 - val_loss: 23.6419\n",
            "Epoch 47/150\n",
            "250/250 [==============================] - 0s 133us/step - loss: 24.1098 - val_loss: 23.6755\n",
            "Epoch 48/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 24.1951 - val_loss: 23.6231\n",
            "Epoch 49/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 24.1209 - val_loss: 23.5928\n",
            "Epoch 50/150\n",
            "250/250 [==============================] - 0s 97us/step - loss: 24.3240 - val_loss: 23.5460\n",
            "Epoch 51/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 24.0978 - val_loss: 23.5732\n",
            "Epoch 52/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 24.0752 - val_loss: 23.6055\n",
            "Epoch 53/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 24.1585 - val_loss: 23.6295\n",
            "Epoch 54/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 24.0779 - val_loss: 23.6524\n",
            "Epoch 55/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 24.2133 - val_loss: 23.7372\n",
            "Epoch 56/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 24.0962 - val_loss: 23.5975\n",
            "Epoch 57/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 24.0286 - val_loss: 23.4593\n",
            "Epoch 58/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 23.8892 - val_loss: 23.4999\n",
            "Epoch 59/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 23.9382 - val_loss: 23.4890\n",
            "Epoch 60/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 24.0785 - val_loss: 23.4243\n",
            "Epoch 61/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 24.0638 - val_loss: 23.4026\n",
            "Epoch 62/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 23.9809 - val_loss: 23.3535\n",
            "Epoch 63/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 24.1099 - val_loss: 23.4511\n",
            "Epoch 64/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 23.9410 - val_loss: 23.6526\n",
            "Epoch 65/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 23.7923 - val_loss: 23.4849\n",
            "Epoch 66/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 23.8030 - val_loss: 23.5628\n",
            "Epoch 67/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 23.7299 - val_loss: 23.2940\n",
            "Epoch 68/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.8196 - val_loss: 23.3911\n",
            "Epoch 69/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 24.0713 - val_loss: 23.4781\n",
            "Epoch 70/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 23.5684 - val_loss: 23.4535\n",
            "Epoch 71/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 23.6942 - val_loss: 23.1745\n",
            "Epoch 72/150\n",
            "250/250 [==============================] - 0s 108us/step - loss: 23.8470 - val_loss: 23.4088\n",
            "Epoch 73/150\n",
            "250/250 [==============================] - 0s 100us/step - loss: 23.6364 - val_loss: 23.3962\n",
            "Epoch 74/150\n",
            "250/250 [==============================] - 0s 116us/step - loss: 23.6504 - val_loss: 23.4153\n",
            "Epoch 75/150\n",
            "250/250 [==============================] - 0s 99us/step - loss: 23.8184 - val_loss: 23.5200\n",
            "Epoch 76/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.8762 - val_loss: 23.3213\n",
            "Epoch 77/150\n",
            "250/250 [==============================] - 0s 98us/step - loss: 23.4561 - val_loss: 23.3899\n",
            "Epoch 78/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.7521 - val_loss: 23.2164\n",
            "Epoch 79/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 23.5561 - val_loss: 23.4743\n",
            "Epoch 80/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.5008 - val_loss: 23.3864\n",
            "Epoch 81/150\n",
            "250/250 [==============================] - 0s 100us/step - loss: 23.6236 - val_loss: 23.2074\n",
            "Epoch 82/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 23.5719 - val_loss: 23.3838\n",
            "Epoch 83/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.5717 - val_loss: 23.2187\n",
            "Epoch 84/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.4403 - val_loss: 23.1351\n",
            "Epoch 85/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 23.3642 - val_loss: 23.2391\n",
            "Epoch 86/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 23.5027 - val_loss: 23.1823\n",
            "Epoch 87/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.3826 - val_loss: 23.1968\n",
            "Epoch 88/150\n",
            "250/250 [==============================] - 0s 108us/step - loss: 23.2412 - val_loss: 23.2528\n",
            "Epoch 89/150\n",
            "250/250 [==============================] - 0s 139us/step - loss: 23.5462 - val_loss: 23.1811\n",
            "Epoch 90/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.5139 - val_loss: 23.2039\n",
            "Epoch 91/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 23.2208 - val_loss: 23.0582\n",
            "Epoch 92/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.3150 - val_loss: 23.0417\n",
            "Epoch 93/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 23.1066 - val_loss: 23.0230\n",
            "Epoch 94/150\n",
            "250/250 [==============================] - 0s 89us/step - loss: 23.3942 - val_loss: 22.9519\n",
            "Epoch 95/150\n",
            "250/250 [==============================] - 0s 101us/step - loss: 23.3436 - val_loss: 23.0049\n",
            "Epoch 96/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 23.4229 - val_loss: 23.3096\n",
            "Epoch 97/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 23.5233 - val_loss: 22.9967\n",
            "Epoch 98/150\n",
            "250/250 [==============================] - 0s 103us/step - loss: 23.3403 - val_loss: 23.0336\n",
            "Epoch 99/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 23.2927 - val_loss: 23.0541\n",
            "Epoch 100/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 23.3248 - val_loss: 23.0685\n",
            "Epoch 101/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 22.8682 - val_loss: 23.1408\n",
            "Epoch 102/150\n",
            "250/250 [==============================] - 0s 105us/step - loss: 23.3556 - val_loss: 23.2190\n",
            "Epoch 103/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.2972 - val_loss: 23.3358\n",
            "Epoch 104/150\n",
            "250/250 [==============================] - 0s 98us/step - loss: 23.2597 - val_loss: 23.2465\n",
            "Epoch 105/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.1302 - val_loss: 23.0361\n",
            "Epoch 106/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 23.0067 - val_loss: 23.0237\n",
            "Epoch 107/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 23.0329 - val_loss: 23.1593\n",
            "Epoch 108/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 23.1021 - val_loss: 22.9402\n",
            "Epoch 109/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 22.9150 - val_loss: 23.0196\n",
            "Epoch 110/150\n",
            "250/250 [==============================] - 0s 92us/step - loss: 23.1647 - val_loss: 23.0390\n",
            "Epoch 111/150\n",
            "250/250 [==============================] - 0s 106us/step - loss: 23.2864 - val_loss: 22.9717\n",
            "Epoch 112/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 23.0562 - val_loss: 23.0689\n",
            "Epoch 113/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 22.9124 - val_loss: 23.2701\n",
            "Epoch 114/150\n",
            "250/250 [==============================] - 0s 97us/step - loss: 22.9878 - val_loss: 23.0433\n",
            "Epoch 115/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.1842 - val_loss: 22.9889\n",
            "Epoch 116/150\n",
            "250/250 [==============================] - 0s 96us/step - loss: 22.8928 - val_loss: 23.1965\n",
            "Epoch 117/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 23.2962 - val_loss: 23.2394\n",
            "Epoch 118/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 23.0139 - val_loss: 22.9899\n",
            "Epoch 119/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 23.0515 - val_loss: 23.1306\n",
            "Epoch 120/150\n",
            "250/250 [==============================] - 0s 114us/step - loss: 22.9309 - val_loss: 23.1244\n",
            "Epoch 121/150\n",
            "250/250 [==============================] - 0s 136us/step - loss: 22.9399 - val_loss: 23.1295\n",
            "Epoch 122/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 22.8305 - val_loss: 22.9094\n",
            "Epoch 123/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 23.0646 - val_loss: 23.1158\n",
            "Epoch 124/150\n",
            "250/250 [==============================] - 0s 102us/step - loss: 23.0606 - val_loss: 22.9157\n",
            "Epoch 125/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 22.8081 - val_loss: 23.0501\n",
            "Epoch 126/150\n",
            "250/250 [==============================] - 0s 98us/step - loss: 22.9867 - val_loss: 22.9953\n",
            "Epoch 127/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 22.9019 - val_loss: 23.1735\n",
            "Epoch 128/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 22.7987 - val_loss: 23.0920\n",
            "Epoch 129/150\n",
            "250/250 [==============================] - 0s 121us/step - loss: 22.8685 - val_loss: 23.0476\n",
            "Epoch 130/150\n",
            "250/250 [==============================] - 0s 132us/step - loss: 22.8021 - val_loss: 22.7777\n",
            "Epoch 131/150\n",
            "250/250 [==============================] - 0s 97us/step - loss: 22.6372 - val_loss: 23.0665\n",
            "Epoch 132/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 22.5678 - val_loss: 23.1204\n",
            "Epoch 133/150\n",
            "250/250 [==============================] - 0s 87us/step - loss: 22.8585 - val_loss: 22.9151\n",
            "Epoch 134/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 22.9301 - val_loss: 23.2329\n",
            "Epoch 135/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 22.3390 - val_loss: 23.1329\n",
            "Epoch 136/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 22.7807 - val_loss: 23.1505\n",
            "Epoch 137/150\n",
            "250/250 [==============================] - 0s 91us/step - loss: 22.6003 - val_loss: 22.7561\n",
            "Epoch 138/150\n",
            "250/250 [==============================] - 0s 90us/step - loss: 22.3789 - val_loss: 22.8567\n",
            "Epoch 139/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 22.5192 - val_loss: 22.9124\n",
            "Epoch 140/150\n",
            "250/250 [==============================] - 0s 93us/step - loss: 22.8904 - val_loss: 22.9199\n",
            "Epoch 141/150\n",
            "250/250 [==============================] - 0s 85us/step - loss: 22.5266 - val_loss: 22.9770\n",
            "Epoch 142/150\n",
            "250/250 [==============================] - 0s 86us/step - loss: 23.0037 - val_loss: 22.8880\n",
            "Epoch 143/150\n",
            "250/250 [==============================] - 0s 84us/step - loss: 22.6647 - val_loss: 23.1446\n",
            "Epoch 144/150\n",
            "250/250 [==============================] - 0s 95us/step - loss: 22.6304 - val_loss: 22.8516\n",
            "Epoch 145/150\n",
            "250/250 [==============================] - 0s 94us/step - loss: 22.4750 - val_loss: 22.7957\n",
            "Epoch 146/150\n",
            "250/250 [==============================] - 0s 88us/step - loss: 22.3606 - val_loss: 23.0708\n",
            "Epoch 147/150\n",
            "250/250 [==============================] - 0s 82us/step - loss: 22.4651 - val_loss: 23.2545\n",
            "Epoch 148/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 22.2153 - val_loss: 22.9899\n",
            "Epoch 149/150\n",
            "250/250 [==============================] - 0s 83us/step - loss: 22.7926 - val_loss: 22.9161\n",
            "Epoch 150/150\n",
            "250/250 [==============================] - 0s 81us/step - loss: 22.4547 - val_loss: 22.6792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0vCRcnhA6l",
        "colab_type": "text"
      },
      "source": [
        "## Generate data with Dropout VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2HndA3MG6LQs",
        "outputId": "701a4eff-15b1-44b7-9629-3f02d6af160c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, original_dim))\n",
        "print(x_test.shape)\n",
        "print(x_test[0].reshape(-1, original_dim).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 58)\n",
            "(1, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxAK5FP5CTyv",
        "colab_type": "text"
      },
      "source": [
        "## Computing time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "61a0c554-0781-459c-998f-ba046288e85a",
        "id": "5jO1WsioCPgX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "computation = []\n",
        "total_nums = 2\n",
        "for _ in range(10):\n",
        "    start = time.time()\n",
        "    results = []\n",
        "    x_test_encoded = vae.predict(x_test,\n",
        "                                 nums=total_nums)\n",
        "    x_test_encoded = np.asarray(x_test_encoded)\n",
        "    results = x_test_encoded\n",
        "    results = np.asarray(results)\n",
        "    results = scaler.inverse_transform(results)\n",
        "    end = time.time()\n",
        "    computation.append(end-start)\n",
        "print(np.mean(computation), np.std(computation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.024167180061340332 0.027872028438232863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sgO9JKSqWsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "total_nums = 2\n",
        "results = []\n",
        "for i in range(x_test.shape[0]):\n",
        "    x_test_encoded = vae.predict(x_test[i].reshape(-1, original_dim), \n",
        "                                 nums=total_nums)\n",
        "    \n",
        "    x_test_encoded = x_test_encoded.reshape(total_nums, original_dim)\n",
        "    results.append(x_test_encoded)\n",
        "results = np.asarray(results)\n",
        "results = results.reshape(total_nums*results.shape[0], original_dim)\n",
        "results = scaler.inverse_transform(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POfJxgHamlmx",
        "colab_type": "text"
      },
      "source": [
        "## Handling Generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YIlGSzekAYlO",
        "colab": {}
      },
      "source": [
        "d = {}\n",
        "names = list(df)\n",
        "for i, name in enumerate(names):\n",
        "    d[name] = results[:, i]\n",
        "df = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkAZ6kCchNII",
        "colab_type": "text"
      },
      "source": [
        "## Re-categoricalize data from Generated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "shurAQCSAYlV",
        "outputId": "82260bab-991f-4750-e1b6-7c1f0044f6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "names = list(df)\n",
        "c_dict = {}\n",
        "for n in names:\n",
        "    if '_' in n:\n",
        "        index = n.index('_')\n",
        "        c_dict[n[:index]] = [c for c in names if n[:index+1] in c]\n",
        "values = []\n",
        "for key, items in c_dict.items():\n",
        "    dummies = df[items]\n",
        "    d_names = list(dummies)\n",
        "    c_dict = {}\n",
        "    for n in d_names:\n",
        "        c_dict[n] = n[n.index('_')+1:]\n",
        "    dummies.rename(columns=c_dict, \n",
        "                   inplace=True)\n",
        "    df[key] = dummies.idxmax(axis=1)\n",
        "    df.drop(items, axis=1, inplace=True)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  return super(DataFrame, self).rename(**kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "          X0        X11       X14       X16       X19  ...   X2   X4   X5   X7    X8\n",
            "0  25.646967  36.278141  1.612389  1.092323  1.528939  ...  A40  A61  A75  A92  A101\n",
            "1  19.446032  31.034742  1.259892  1.087729  1.262563  ...  A43  A61  A73  A94  A101\n",
            "2  23.790070  35.311882  1.587378  1.292253  1.226454  ...  A49  A61  A73  A93  A101\n",
            "3  18.309425  36.927261  1.755193  1.321804  1.228635  ...  A43  A61  A73  A93  A101\n",
            "4  24.141148  35.124191  1.367912  1.032213  1.157195  ...  A43  A61  A73  A93  A101\n",
            "\n",
            "[5 rows x 20 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8dLIoyVW_Kn",
        "colab": {}
      },
      "source": [
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "df.to_csv(path + '_dropout.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL6N8v2sYTNm",
        "colab_type": "text"
      },
      "source": [
        "# Encoding categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgVhmliYYzwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(path + '_For_Test.csv',\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "df_mc = pd.read_csv(path + '_dropout.csv',\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "df_vae = pd.read_csv(path + '_vae.csv',\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "names = list(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSbyWbb0ZjAJ",
        "colab_type": "code",
        "outputId": "9f2d8d0e-3de6-48df-b15b-b03884e2840d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "colnums = len(df.columns)\n",
        "for i in df.columns:\n",
        "    try:\n",
        "        if df[i].dtype.name == 'object':\n",
        "            df[i] = df[i].astype('category')\n",
        "    except:\n",
        "        continue\n",
        "cat_columns = df.select_dtypes(['category']).columns\n",
        "print(cat_columns)\n",
        "for col in cat_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].values)\n",
        "    df_mc[col] = le.transform(df_mc[col].values)\n",
        "    df_vae[col] = le.transform(df_vae[col].values)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['X1', 'X10', 'X12', 'X13', 'X15', 'X17', 'X18', 'X2', 'X4', 'X5', 'X7',\n",
            "       'X8'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-scTKTHZ9Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "df_mc = df_mc.reindex(sorted(df_mc.columns), axis=1)\n",
        "df_vae = df_vae.reindex(sorted(df_vae.columns), axis=1)\n",
        "df.to_csv(path + '_For_Test_encoded.csv')\n",
        "df_mc.to_csv(path + '_dropout_encoded.csv')\n",
        "df_vae.to_csv(path + '_vae_encoded.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}